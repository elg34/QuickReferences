{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "data = pd.DataFrame(data=datasets.load_iris().data, columns=['seplen','sepwid','petlen','petwid'])\n",
    "data['target']=datasets.load_iris().target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[data.columns[:-1]],data['target'],test_size=0.2,random_state=23,stratify=data['target'])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scal = StandardScaler()\n",
    "x_train = scal.fit_transform(x_train)\n",
    "x_test = scal.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C': [0.01,0.1,1,10]}\n",
    "clf = GridSearchCV(SVC(),parameters,cv=5)\n",
    "clf.fit(x_train,y_train)\n",
    "print(clf.best_params_)\n",
    "y_true,y_pred=y_test,clf.predict(x_test)\n",
    "print(classification_report(y_true,y_pred))\n",
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22178881  0.2363588   0.52710905]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xc11e3c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data[data.columns[0:3]],data[data.columns[3]],test_size=0.2,random_state=23)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train,y_train)\n",
    "print(clf.coef_)\n",
    "y_true,y_pred = y_test,clf.predict(x_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[data.columns[:-1]],data['target'],test_size=0.2,random_state=23,stratify=data['target'])\n",
    "y_train = to_categorical(y_train, num_classes=len(set(data['target'])))\n",
    "y_test = to_categorical(y_test, num_classes=len(set(data['target'])))\n",
    "scal = StandardScaler()\n",
    "x_train = scal.fit_transform(x_train)\n",
    "x_test = scal.transform(x_test)\n",
    "          \n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(data['target'])), activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.9744 - acc: 0.5667\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.8715 - acc: 0.7250\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.7308 - acc: 0.7833\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.6709 - acc: 0.7917\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.6371 - acc: 0.7833\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.5734 - acc: 0.8083\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.5014 - acc: 0.8000\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.4912 - acc: 0.8250\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.4598 - acc: 0.8167\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.4206 - acc: 0.8167\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.4062 - acc: 0.8333\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3856 - acc: 0.8250\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3770 - acc: 0.8333\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3526 - acc: 0.8583\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3442 - acc: 0.8500\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3308 - acc: 0.8417\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3218 - acc: 0.8250\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3098 - acc: 0.8667\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3166 - acc: 0.8583\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.3064 - acc: 0.8500\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2806 - acc: 0.8667\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2880 - acc: 0.8833\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2761 - acc: 0.9083\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 0s 58us/step - loss: 0.2548 - acc: 0.8750\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2715 - acc: 0.9000\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2736 - acc: 0.8417\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2537 - acc: 0.9000\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2372 - acc: 0.8833\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2263 - acc: 0.9250\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2447 - acc: 0.9167\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2454 - acc: 0.9167\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2290 - acc: 0.9333\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2455 - acc: 0.8833\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2008 - acc: 0.9167\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2011 - acc: 0.9417\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2198 - acc: 0.9167\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2220 - acc: 0.9583\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2223 - acc: 0.9167\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2206 - acc: 0.8917\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.2192 - acc: 0.9250\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1864 - acc: 0.9333\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1694 - acc: 0.9333\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1975 - acc: 0.9417\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.2008 - acc: 0.9167\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1931 - acc: 0.9250\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1892 - acc: 0.9417\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.1879 - acc: 0.937 - 0s 50us/step - loss: 0.1493 - acc: 0.9500\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1566 - acc: 0.9500\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1612 - acc: 0.9333\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1428 - acc: 0.9667\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1653 - acc: 0.9250\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1559 - acc: 0.9500\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1527 - acc: 0.9417\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1509 - acc: 0.9583\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1387 - acc: 0.9500\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.1747 - acc: 0.9500\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1325 - acc: 0.9583\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.1511 - acc: 0.9333\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1406 - acc: 0.9583\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1315 - acc: 0.9500\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1554 - acc: 0.9333\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1452 - acc: 0.9667\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1144 - acc: 0.9583\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1721 - acc: 0.9333\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.1155 - acc: 0.9667\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1063 - acc: 0.9750\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1181 - acc: 0.9500\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.1349 - acc: 0.9583\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1596 - acc: 0.9333\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1084 - acc: 0.9500\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1368 - acc: 0.9500\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1032 - acc: 0.9500\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1406 - acc: 0.9500\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1075 - acc: 0.9500\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1030 - acc: 0.9583\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1306 - acc: 0.9333\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0967 - acc: 0.9667\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0953 - acc: 0.9750\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1320 - acc: 0.9583\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1132 - acc: 0.9583\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1140 - acc: 0.9500\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1046 - acc: 0.9667\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0895 - acc: 0.9750\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 42us/step - loss: 0.1121 - acc: 0.9500\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1297 - acc: 0.9500\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0920 - acc: 0.9667\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0798 - acc: 0.9917\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0979 - acc: 0.9667\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0848 - acc: 0.9583\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0935 - acc: 0.9667\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0878 - acc: 0.9500\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0908 - acc: 0.9750\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1213 - acc: 0.9500\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1037 - acc: 0.9750\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0996 - acc: 0.9500\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0972 - acc: 0.9583\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0858 - acc: 0.9667\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0969 - acc: 0.9667\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0708 - acc: 0.9667\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.0689 - acc: 0.9750\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1095 - acc: 0.9667\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.1090 - acc: 0.9500\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1049 - acc: 0.9583\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1456 - acc: 0.9417\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.1712 - acc: 0.937 - 0s 42us/step - loss: 0.1080 - acc: 0.9750\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0713 - acc: 0.9750\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1100 - acc: 0.9500\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1412 - acc: 0.9583\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0848 - acc: 0.9667\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0887 - acc: 0.9750\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0828 - acc: 0.9667\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0931 - acc: 0.9500\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0789 - acc: 0.9750\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0805 - acc: 0.9667\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.0716 - acc: 0.9667\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0823 - acc: 0.9583\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0846 - acc: 0.9500\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.0668 - acc: 0.9667\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0776 - acc: 0.9667\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0647 - acc: 0.9667\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0852 - acc: 0.9667\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0875 - acc: 0.9667\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0561 - acc: 0.9750\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0701 - acc: 0.9583\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0667 - acc: 0.9667\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.1101 - acc: 0.9583\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0519 - acc: 0.9833\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0627 - acc: 0.9750\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0770 - acc: 0.9750\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0668 - acc: 0.9667\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0826 - acc: 0.9750\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0617 - acc: 0.9667\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0887 - acc: 0.9667\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0845 - acc: 0.9583\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0628 - acc: 0.9667\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0758 - acc: 0.9500\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.0983 - acc: 0.9750\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0706 - acc: 0.9667\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0610 - acc: 0.9750\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0758 - acc: 0.9750\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0753 - acc: 0.9667\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0634 - acc: 0.9833\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0690 - acc: 0.9750\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0675 - acc: 0.9750\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0746 - acc: 0.9833\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0713 - acc: 0.9750\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0667 - acc: 0.9750\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0532 - acc: 0.9750\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0529 - acc: 0.9833\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0788 - acc: 0.9667\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0912 - acc: 0.9667\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0423 - acc: 0.9833\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0642 - acc: 0.9750\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0457 - acc: 0.9750\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0802 - acc: 0.9667\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0411 - acc: 0.9917\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0518 - acc: 0.9750\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0536 - acc: 0.9833\n",
      "Epoch 159/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0829 - acc: 0.9667\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0766 - acc: 0.9667\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0701 - acc: 0.9667\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0420 - acc: 0.9917\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0896 - acc: 0.9667\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0543 - acc: 0.9667\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0808 - acc: 0.9750\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 42us/step - loss: 0.0461 - acc: 0.9917\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0520 - acc: 0.9667\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1055 - acc: 0.9583\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0488 - acc: 0.9750\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0734 - acc: 0.9583\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0785 - acc: 0.9667\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0878 - acc: 0.9583\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0384 - acc: 0.9917\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0402 - acc: 0.9833\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0629 - acc: 0.9750\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0778 - acc: 0.9833\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0547 - acc: 1.000 - 0s 42us/step - loss: 0.0666 - acc: 0.9833\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0637 - acc: 0.9667\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0688 - acc: 0.9750\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0736 - acc: 0.9583\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0590 - acc: 0.9750\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0389 - acc: 0.9833\n",
      "Epoch 183/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0462 - acc: 0.9833\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0534 - acc: 0.9833\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0592 - acc: 0.9667\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0496 - acc: 0.9750\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.0537 - acc: 0.9667\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0840 - acc: 0.9667\n",
      "Epoch 189/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0634 - acc: 0.9667\n",
      "Epoch 190/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0528 - acc: 0.9750\n",
      "Epoch 191/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0460 - acc: 0.9750\n",
      "Epoch 192/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0533 - acc: 0.9833\n",
      "Epoch 193/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0347 - acc: 0.9833\n",
      "Epoch 194/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0528 - acc: 0.9833\n",
      "Epoch 195/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.1056 - acc: 0.9583\n",
      "Epoch 196/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0690 - acc: 0.9667\n",
      "Epoch 197/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0611 - acc: 0.9750\n",
      "Epoch 198/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0707 - acc: 0.9667\n",
      "Epoch 199/200\n",
      "120/120 [==============================] - 0s 50us/step - loss: 0.0484 - acc: 0.9833\n",
      "Epoch 200/200\n",
      "120/120 [==============================] - 0s 42us/step - loss: 0.0691 - acc: 0.9750\n",
      "30/30 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=200)\n",
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03331363946199417, 1.0]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
